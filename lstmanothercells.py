import pandas as pd
import tensorflow as tf
import numpy as np
from tensorflow.keras.layers import LSTM, Activation, Dropout, Dense, Input, Conv1D, MaxPooling1D, GlobalMaxPooling1D
from tensorflow.keras.models import Model
import string
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from sklearn.preprocessing import LabelBinarizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
import tensorflow.keras
from sklearn.model_selection import train_test_split
import pandas as pd
from gensim.models import KeyedVectors



from keras.models import Sequential
from keras.layers import *
import gensim
import sys
import numpy as np
import pandas as pd
import string
import logging
import random

data = pd.read_csv("final.csv")
data.dropna(inplace = True)
data.reset_index(inplace = True,drop = True)
reloaded_word_vectors = KeyedVectors.load('vectors_v2.kv')
maxLen = 14638

for x in data.index:
      if data.loc[x, "index"] > 132:
              data.drop(x, inplace = True)


embed_vector_len = reloaded_word_vectors["SHL"].shape[0]
vocab_len = len(reloaded_word_vectors.index_to_key)
emb_matrix = np.zeros((vocab_len, embed_vector_len))


for index, word in enumerate(reloaded_word_vectors.index_to_key):
  embedding_vector = reloaded_word_vectors[word]
  if embedding_vector is not None:
    emb_matrix[index, :] = embedding_vector

embedding_layer = tf.keras.layers.Embedding(input_dim=vocab_len, output_dim=embed_vector_len, input_length=maxLen, weights = [emb_matrix], trainable=False)

def adjust_hex(row):
    array = row.contract_code.split()
    result_string = ""
    for code in array:
        if("0x" in code):
            continue
        elif("PUSH" in code):
            result_string  = result_string + "PUSH" + " "
        elif("SWAP" in code):
            result_string  = result_string + "SWAP" + " "
        elif("DUP" in code):
            result_string  = result_string + "DUP" + " "
        elif("LOG" in code):
            result_string  = result_string + "LOG" + " "
        else:
            result_string  = result_string + code + " "
        
    return result_string.strip()

data["opcode_adjusted"] = data.apply(lambda row:adjust_hex(row),axis = 1)


reviews = data["opcode_adjusted"]

reviews_list = []
for i in range(len(reviews)):
  reviews_list.append(reviews[i])


sentiment = data['vulnerability']
unique_labels = np.unique(sentiment)
label_numbers = np.searchsorted(unique_labels, sentiment)


X_train, X_test,Y_train, Y_test = train_test_split(reviews_list, label_numbers, test_size=0.2, random_state = 45)


tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(X_train)
words_to_index = tokenizer.word_index


def find_length(row):
    return len(row["opcode_adjusted"].split())





embed_vector_len = reloaded_word_vectors["SHL"].shape[0]
vocab_len = len(reloaded_word_vectors.index_to_key)
emb_matrix = np.zeros((vocab_len, embed_vector_len))
n_categories = len(unique_labels)


w2v_weights = reloaded_word_vectors.vectors
X_train, X_test,Y_train, Y_test = train_test_split(reviews_list, label_numbers, test_size=0.2, random_state = 45)


a = reloaded_word_vectors.key_to_index

train_indices = []
for i in X_train:
    array = i.split()
    result_array=[]
    for i in range(len(array)):
        if(array[i] == 'EXTCODEHASH'):
            continue
        result_array.append(a[array[i]])
    train_indices.append(result_array)



model = Sequential()

# Keras Embedding layer with Word2Vec weights initialization
model.add(Embedding(input_dim=vocab_len,
                    output_dim=embed_vector_len,
                    weights=[w2v_weights],
                    input_length=maxLen,
                    mask_zero=True,
                    trainable=False))

model.add(Bidirectional(LSTM(100)))
model.add(Dense(n_categories, activation='softmax'))

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])
X_train_indices = pad_sequences(train_indices, maxlen=maxLen, padding='post')

history = model.fit(X_train_indices, Y_train, batch_size=16, epochs=15)
